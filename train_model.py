
"""
ëª¨ë¸ í›ˆë ¨ ë° ì €ì¥ ìŠ¤í¬ë¦½íŠ¸
LightGBM ë˜ëŠ” Transformer ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
"""
import geopandas as gpd
import pandas as pd
import os
import joblib
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import TimeSeriesSplit
from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler

from src.load_data import load_all_csv
from src.geocoding import add_lat_lon_from_address
from src.model import add_fire_label

class WildfireTransformer(nn.Module):
    """
    ê°„ë‹¨í•œ ì¸ì½”ë” ê¸°ë°˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸.
    ê³¼ê±° ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ë°›ì•„ ë¯¸ë˜ì˜ ì‚°ë¶ˆ ë°œìƒ í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    def __init__(self, n_features, n_heads, n_layers, dropout, pred_len):
        super().__init__()
        self.n_features = n_features
        self.pred_len = pred_len

        # ì¸ì½”ë” ë ˆì´ì–´ ì •ì˜
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=n_features, 
            nhead=n_heads, 
            dropout=dropout, 
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        # ìµœì¢… ì˜ˆì¸¡ì„ ìœ„í•œ ì„ í˜• ë ˆì´ì–´
        self.output_layer = nn.Linear(n_features, pred_len)

    def forward(self, src):
        # src: (batch_size, seq_len, n_features)
        encoded = self.transformer_encoder(src)
        # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…ì˜ ì¶œë ¥ì„ ì‚¬ìš©í•˜ì—¬ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡
        output = self.output_layer(encoded[:, -1, :]) # (batch_size, pred_len)
        return output


def create_sequences(data, seq_length, pred_length):
    """
    ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    X: (seq_length) ê¸¸ì´ì˜ ê³¼ê±° ë°ì´í„°, y: (pred_length) ê¸¸ì´ì˜ ë¯¸ë˜ ë°ì´í„°
    """
    xs, ys = [], []
    for i in range(len(data) - seq_length - pred_length + 1):
        x = data[i:(i + seq_length)]
        y = data[(i + seq_length):(i + seq_length + pred_length), -1] # íƒ€ê²Ÿì€ ë§ˆì§€ë§‰ 'IS_FIRE' ì»¬ëŸ¼
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)


def train_and_save_lightgbm_model(df, model_path="models/fire_predictor.joblib"):
    """
    ë°ì´í„°ë¡œ LightGBM ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•œ ë’¤, ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµí•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print("âœ¨ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    
    datetime_cols = df[['ë°œìƒì¼ì‹œ_ë…„', 'ë°œìƒì¼ì‹œ_ì›”', 'ë°œìƒì¼ì‹œ_ì¼']].copy()
    datetime_cols.columns = ['year', 'month', 'day']
    df['datetime'] = pd.to_datetime(datetime_cols, errors='coerce')
    df.dropna(subset=['datetime'], inplace=True)
    df = df.sort_values('datetime').reset_index(drop=True)

    features = ['LAT', 'LON', 'ì›”', 'ìš”ì¼', 'í”¼í•´ë©´ì _í•©ê³„'] 
    target = 'IS_FIRE'

    for col in features:
        if df[col].isnull().any():
            df[col] = df[col].fillna(df[col].median())

    X = df[features]
    y = df[target]

    print("â³ ì‹œê³„ì—´ êµì°¨ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì¤‘...")
    tscv = TimeSeriesSplit(n_splits=5)
    for train_index, test_index in tscv.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ")

    print("ğŸš€ ëª¨ë¸ í›ˆë ¨ ì¤‘ (LightGBM)...")
    lgbm = LGBMClassifier(objective='binary', is_unbalance=True, random_state=42)
    lgbm.fit(X_train, y_train)

    print("ğŸ“Š ëª¨ë¸ í‰ê°€ ê²°ê³¼:")
    y_pred = lgbm.predict(X_test)
    print(classification_report(y_test, y_pred, target_names=['No Fire', 'Fire'], zero_division=0, labels=[0, 1]))

    print("ğŸ’¾ ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ë° ì €ì¥ ì¤‘...")
    final_model = LGBMClassifier(objective='binary', is_unbalance=True, random_state=42)
    final_model.fit(X, y)

    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    joblib.dump(final_model, model_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")


def train_and_save_transformer(
    df, 
    model_path="models/transformer_predictor.pth", 
    scaler_path="models/transformer_scaler.joblib"
):
    """íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
    print("ğŸ¤– íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")

    features = ['LAT', 'LON', 'ì›”', 'ìš”ì¼', 'í”¼í•´ë©´ì _í•©ê³„', 'IS_FIRE']
    datetime_cols = df[['ë°œìƒì¼ì‹œ_ë…„', 'ë°œìƒì¼ì‹œ_ì›”', 'ë°œìƒì¼ì‹œ_ì¼']].copy()
    datetime_cols.columns = ['year', 'month', 'day']
    df['datetime'] = pd.to_datetime(datetime_cols, errors='coerce')
    df.dropna(subset=['datetime'], inplace=True)
    df = df.sort_values('datetime').reset_index(drop=True)
    
    df_model = df[features].fillna(0)

    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(df_model)

    SEQ_LENGTH = 30
    PRED_LENGTH = 7
    X, y = create_sequences(data_scaled, SEQ_LENGTH, PRED_LENGTH)

    X_tensor = torch.FloatTensor(X)
    y_tensor = torch.FloatTensor(y)

    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    model = WildfireTransformer(
        n_features=len(features), 
        n_heads=len(features),
        n_layers=2, 
        dropout=0.1, 
        pred_len=PRED_LENGTH
    ).to(device)

    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    epochs = 20
    print(f"ğŸš€ {epochs} ì—í¬í¬ ë™ì•ˆ í›ˆë ¨ ì§„í–‰...")
    for epoch in range(epochs):
        model.train()
        for batch_X, batch_y in dataloader:
            batch_X, batch_y = batch_X.to(device), batch_y.to(device)

            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            loss.backward()
            optimizer.step()
        
        if (epoch + 1) % 5 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

    os.makedirs(os.path.dirname(model_path), exist_ok=True)
    torch.save(model.state_dict(), model_path)
    joblib.dump(scaler, scaler_path)
    print(f"âœ… íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    print(f"âœ… ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ: {scaler_path}")

if __name__ == "__main__":
    # ëª¨ë¸ ì„ íƒ
    MODEL_TYPE = "lightgbm" # "lightgbm" ë˜ëŠ” "transformer"

    geojson_path = "data/true_fires.geojson"
    df = gpd.read_file(geojson_path)


    df['IS_FIRE'] = (df['í”¼í•´ë©´ì _í•©ê³„'] > 0).astype(int)
    
    day_map = {'ì›”': 0, 'í™”': 1, 'ìˆ˜': 2, 'ëª©': 3, 'ê¸ˆ': 4, 'í† ': 5, 'ì¼': 6}
    df['ìš”ì¼'] = df['ë°œìƒì¼ì‹œ_ìš”ì¼'].map(day_map)

    if MODEL_TYPE == "lightgbm":
        train_and_save_lightgbm_model(df)
    elif MODEL_TYPE == "transformer":
        train_and_save_transformer(df)
    else:
        print(f"ğŸš¨ ì˜ëª»ëœ ëª¨ë¸ íƒ€ì…ì…ë‹ˆë‹¤: {MODEL_TYPE}")
